{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13170314,"sourceType":"datasetVersion","datasetId":8324170}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install evaluate\nimport pandas as pd\nimport numpy as np\nimport torch\nfrom datasets import Dataset, DatasetDict\nimport evaluate\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer, DataCollatorForTokenClassification\nfrom tqdm.auto import tqdm\nimport os\nimport glob\nimport ast\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Используется устройство: {device}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-25T22:03:40.712828Z","iopub.execute_input":"2025-09-25T22:03:40.713422Z","iopub.status.idle":"2025-09-25T22:03:44.536187Z","shell.execute_reply.started":"2025-09-25T22:03:40.713395Z","shell.execute_reply":"2025-09-25T22:03:44.535271Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.6)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.6.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.3)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.4)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.33.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (25.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.18.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (19.0.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.12.13)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.14.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.1.5)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.6.15)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.7.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.6.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.2)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->evaluate) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->evaluate) (2024.2.0)\nИспользуется устройство: cuda\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"DATA_DIR = \"/kaggle/input/avito-task-segment/\"\nTOTAL_TRAIN_SAMPLES = 3_000_000 \nMODEL_CHECKPOINT = \"Geotrend/distilbert-base-ru-cased\"\nMAX_LEN = 128\nRANDOM_SEED = 42\n\nprint(\"Этап 0: Загрузка всех данных в Pandas DataFrame...\")\ntrain_files = glob.glob(f\"{DATA_DIR}train_*.parquet\")\ndf_list = []\nsamples_per_file = TOTAL_TRAIN_SAMPLES // len(train_files) if train_files else 0\nfor file in tqdm(train_files, desc=\"Чтение Parquet файлов\"):\n    df = pd.read_parquet(file)\n    sample_df = df.sample(n=min(samples_per_file, len(df)), random_state=RANDOM_SEED)\n    df_list.append(sample_df)\nfull_df = pd.concat(df_list, ignore_index=True)\nprint(f\"Загружено {len(full_df)} строк.\")\n\nprint(\"\\nЭтап 0.5: Очистка и фильтрация данных...\")\ninitial_size = len(full_df)\nfull_df.drop_duplicates(subset=['sentence_without_spaces'], inplace=True, keep='first')\nprint(f\"Удалено {initial_size - len(full_df)} дубликатов.\")\n\ninitial_size = len(full_df)\nfull_df = full_df[full_df['true_positions'].str.len() > 0]\nprint(f\"Отфильтровано {initial_size - len(full_df)} строк без пробелов. Осталось {len(full_df)} ПОЛЕЗНЫХ строк.\")\n\nfull_train_dataset_raw = Dataset.from_pandas(full_df)\ndel full_df, df_list\ndef create_char_level_labels(example):\n    text_no_spaces = example[\"sentence_without_spaces\"]\n    true_positions = example[\"true_positions\"] \n    \n    if not isinstance(text_no_spaces, str) or not text_no_spaces:\n        return {'tokens': [], 'labels': []}\n    \n    tokens = list(text_no_spaces)\n    labels = [0] * len(tokens)\n    \n    for pos in true_positions:\n        if 0 < pos <= len(labels):\n            labels[pos - 1] = 1\n    return {'tokens': tokens, 'labels': labels}\n\nsplit_dataset = full_train_dataset_raw.train_test_split(test_size=0.1, seed=RANDOM_SEED)\ntokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n\ndef tokenize_and_align_labels(examples):\n    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True, max_length=MAX_LEN)\n    labels = []\n    for i, label in enumerate(examples[\"labels\"]):\n        word_ids = tokenized_inputs.word_ids(batch_index=i)\n        previous_word_idx = None\n        label_ids = []\n        for word_idx in word_ids:\n            if word_idx is None: label_ids.append(-100)\n            elif word_idx != previous_word_idx: label_ids.append(label[word_idx])\n            else: label_ids.append(-100)\n            previous_word_idx = word_idx\n        labels.append(label_ids)\n    tokenized_inputs[\"labels\"] = labels\n    return tokenized_inputs\n\nprint(\"\\nЭтап 1: Создание СИМВОЛЬНЫХ токенов и меток...\")\nprocessed_split = split_dataset.map(create_char_level_labels, remove_columns=split_dataset[\"train\"].column_names)\nprint(\"\\nЭтап 2: Токенизация и выравнивание меток...\")\nfinal_dataset = processed_split.map(tokenize_and_align_labels, batched=True)\nfinal_dataset['validation'] = final_dataset.pop('test')\n\nprint(\"\\nДанные готовы.\")\nprint(f\"Размер обучающей выборки: {len(final_dataset['train'])}\")\nprint(f\"Размер валидационной выборки: {len(final_dataset['validation'])}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T22:03:44.537862Z","iopub.execute_input":"2025-09-25T22:03:44.538119Z","iopub.status.idle":"2025-09-25T22:20:32.213888Z","shell.execute_reply.started":"2025-09-25T22:03:44.538094Z","shell.execute_reply":"2025-09-25T22:20:32.213132Z"}},"outputs":[{"name":"stdout","text":"Этап 0: Загрузка всех данных в Pandas DataFrame...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Чтение Parquet файлов:   0%|          | 0/28 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d2d68f0fe7d4ee797f1b97bac587eca"}},"metadata":{}},{"name":"stdout","text":"Загружено 2901223 строк.\n\nЭтап 0.5: Очистка и фильтрация данных...\nУдалено 733777 дубликатов.\nОтфильтровано 59583 строк без пробелов. Осталось 2107863 ПОЛЕЗНЫХ строк.\n\nЭтап 1: Создание СИМВОЛЬНЫХ токенов и меток...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1897076 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"017d6dc8e9e0461ca1998b3d791458bf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/210787 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7feff9087134e83a1a9d78d3fca6e5a"}},"metadata":{}},{"name":"stdout","text":"\nЭтап 2: Токенизация и выравнивание меток...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1897076 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5cc724b23bc34517a45cececb50622e4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/210787 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38b16ff75abb49ab8c5069a6f13d08f2"}},"metadata":{}},{"name":"stdout","text":"\nДанные готовы.\nРазмер обучающей выборки: 1897076\nРазмер валидационной выборки: 210787\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"print(\"\\nПримеры из обучающей выборки (первые 5 строк):\")\nfor i in range(5):\n    example = final_dataset[\"train\"][i]\n    print(f\"Пример {i+1}: Токены = {example['tokens']}, Метки = {example['labels']}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T22:20:32.214678Z","iopub.execute_input":"2025-09-25T22:20:32.214910Z","iopub.status.idle":"2025-09-25T22:20:32.221472Z","shell.execute_reply.started":"2025-09-25T22:20:32.214892Z","shell.execute_reply":"2025-09-25T22:20:32.220750Z"}},"outputs":[{"name":"stdout","text":"\nПримеры из обучающей выборки (первые 5 строк):\nПример 1: Токены = ['С', 'е', 'н', 'с', 'о', 'р', 'M', 'u', 'l', 't', 'i', 't', 'o', 'u', 'c', 'h', '.'], Метки = [-100, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100]\nПример 2: Токены = ['Т', 'а', 'к', 'ж', 'е', 'о', 'т', 'д', 'а', 'м', 'п', 'о', 'л', 'н', 'ы', 'й', 'к', 'о', 'м', 'п', 'л', 'е', 'к', 'т', 'и', 'ч', 'е', 'х', 'о', 'л', 'в', 'п', 'о', 'д', 'а', 'р', 'о', 'к', '.'], Метки = [-100, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, -100]\nПример 3: Токены = ['К', 'в', 'а', 'р', 'т', 'и', 'р', 'а', 'в', 'п', 'р', 'е', 'д', 'ч', 'и', 'с', 'т', 'о', 'в', 'о', 'й', 'о', 'т', 'д', 'е', 'л', 'к', 'е', '.'], Метки = [-100, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, -100]\nПример 4: Токены = ['в', 'с', 'ё', 'э', 'т', 'о', 'п', 'р', 'и', 'х', 'о', 'р', 'а', 'ш', 'и', 'в', 'а', 'н', 'и', 'е', 'и', 'т', 'щ', 'е', 'с', 'л', 'а', 'в', 'и', 'е', '?'], Метки = [-100, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100]\nПример 5: Токены = ['Н', 'о', 'в', 'о', 'е', 'к', 'а', 'ч', 'е', 'с', 'т', 'в', 'е', 'н', 'н', 'о', 'е', 'с', 'т', 'и', 'л', 'ь', 'н', 'о', 'е', 'п', 'а', 'л', 'ь', 'т', 'о'], Метки = [-100, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, -100]\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"from collections import Counter\nimport numpy as np\n\nprint(\"--- Вычисляем веса классов для борьбы с дисбалансом ---\")\n\nlabels_counter = Counter()\nfor item in tqdm(final_dataset[\"train\"], desc=\"Подсчет меток\"):\n    labels_counter.update(item['labels'])\n\ndel labels_counter[-100]\n\ncount_no_space = labels_counter[0]\ncount_space = labels_counter[1]\ntotal = count_no_space + count_space\n\nprint(f\"Всего меток 'NO_SPACE' (0): {count_no_space}\")\nprint(f\"Всего меток 'SPACE' (1):    {count_space}\")\n\nweight_no_space = total / (2 * count_no_space)\nweight_space = total / (2 * count_space)\n\nclass_weights = torch.tensor([weight_no_space, weight_space], dtype=torch.float).to(device)\n\nprint(f\"\\nВычисленные веса: [NO_SPACE: {weight_no_space:.2f}, SPACE: {weight_space:.2f}]\")\nprint(\"Это означает, что ошибка на метке 'SPACE' будет 'штрафоваться' сильнее.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T22:20:32.223245Z","iopub.execute_input":"2025-09-25T22:20:32.223698Z","iopub.status.idle":"2025-09-25T22:25:36.810332Z","shell.execute_reply.started":"2025-09-25T22:20:32.223675Z","shell.execute_reply":"2025-09-25T22:25:36.807253Z"}},"outputs":[{"name":"stdout","text":"--- Вычисляем веса классов для борьбы с дисбалансом ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Подсчет меток:   0%|          | 0/1897076 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"774762fd843846989b42b9abf5418309"}},"metadata":{}},{"name":"stdout","text":"Всего меток 'NO_SPACE' (0): 73872622\nВсего меток 'SPACE' (1):    12503960\n\nВычисленные веса: [NO_SPACE: 0.58, SPACE: 3.45]\nЭто означает, что ошибка на метке 'SPACE' будет 'штрафоваться' сильнее.\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"!pip install seqeval","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T22:25:36.812406Z","iopub.execute_input":"2025-09-25T22:25:36.812627Z","iopub.status.idle":"2025-09-25T22:25:41.424923Z","shell.execute_reply.started":"2025-09-25T22:25:36.812602Z","shell.execute_reply":"2025-09-25T22:25:41.424141Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: seqeval in /usr/local/lib/python3.11/dist-packages (1.2.2)\nRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from seqeval) (1.26.4)\nRequirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.11/dist-packages (from seqeval) (1.2.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->seqeval) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->seqeval) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->seqeval) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->seqeval) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->seqeval) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->seqeval) (2.4.1)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.15.3)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.5.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.6.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.14.0->seqeval) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.14.0->seqeval) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.14.0->seqeval) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.14.0->seqeval) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.14.0->seqeval) (2024.2.0)\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"from transformers import EarlyStoppingCallback\nimport torch.nn as nn\n\nclass CustomTrainer(Trainer):\n    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n        labels = inputs.pop(\"labels\")\n        outputs = model(**inputs)\n        logits = outputs.get(\"logits\")\n        loss_fct = nn.CrossEntropyLoss(weight=class_weights)\n        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n        return (loss, outputs) if return_outputs else loss\n\nmodel = AutoModelForTokenClassification.from_pretrained(MODEL_CHECKPOINT, num_labels=2)\nmodel.to(device)\n\nmetric = evaluate.load(\"seqeval\")\nlabel_names = [\"NO_SPACE\", \"SPACE\"]\ndef compute_metrics(p):\n    predictions, labels = p\n    predictions = np.argmax(predictions, axis=2)\n    true_predictions = [[label_names[p] for (p, l) in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels)]\n    true_labels = [[label_names[l] for (p, l) in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels)]\n    results = metric.compute(predictions=true_predictions, references=true_labels, zero_division=0)\n    return {\"precision\": results[\"overall_precision\"], \"recall\": results[\"overall_recall\"], \"f1\": results[\"overall_f1\"], \"accuracy\": results[\"overall_accuracy\"]}\n\ntraining_args = TrainingArguments(\n    output_dir=f\"./results-{MODEL_CHECKPOINT.replace('/', '_')}\",\n    logging_strategy=\"epoch\",\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    learning_rate=3e-5,\n    num_train_epochs=4,\n    weight_decay=0.01,\n    fp16=True,\n    per_device_train_batch_size=32,\n    per_device_eval_batch_size=32,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_f1\",\n    report_to=\"none\",\n    save_total_limit=1\n)\n\ndata_collator = DataCollatorForTokenClassification(tokenizer)\n\ntrainer = CustomTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=final_dataset[\"train\"],\n    eval_dataset=final_dataset[\"validation\"],\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n)\n\nprint(f\"\\nНачинаем fine-tuning модели '{MODEL_CHECKPOINT}' с использованием весов классов...\")\ntrainer.train()\nprint(\"\\nОбучение завершено.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T22:25:53.307811Z","iopub.execute_input":"2025-09-25T22:25:53.308478Z"}},"outputs":[{"name":"stderr","text":"Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at Geotrend/distilbert-base-ru-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipykernel_36/1726884055.py:45: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n  trainer = CustomTrainer(\n","output_type":"stream"},{"name":"stdout","text":"\nНачинаем fine-tuning модели 'Geotrend/distilbert-base-ru-cased' с использованием весов классов...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='29' max='237136' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [    29/237136 00:04 < 11:48:56, 5.57 it/s, Epoch 0.00/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"import torch.nn.functional as F\n\ndef debug_restore_positions(text, model, tokenizer, device, threshold=0.5):\n    print(f\"--- Отладка для строки: '{text}' ---\")\n    if not isinstance(text, str) or not text.strip():\n        return []\n\n    tokens = list(text)\n    inputs = tokenizer(\n        tokens,\n        is_split_into_words=True,\n        return_tensors=\"pt\",\n        max_length=MAX_LEN,\n        padding=\"max_length\",\n        truncation=True\n    )\n    word_ids_map = inputs.word_ids()\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n\n    model.eval()\n    with torch.no_grad():\n        logits = model(**inputs).logits[0]\n    probabilities = F.softmax(logits, dim=1).cpu().numpy()\n    predictions_argmax = torch.argmax(logits, dim=1).cpu().numpy()\n\n    print(\"Индекс | Символ | Логиты (NO_SPACE, SPACE) | Вероятности (NO, YES) | Argmax | Решение (>= порога)\")\n    print(\"-\" * 80)\n    \n    positions = []\n    \n    for i, word_idx in enumerate(word_ids_map):\n        if word_idx is not None:\n            char = tokens[word_idx]\n            logit_vals = logits[i].cpu().numpy()\n            prob_vals = probabilities[i]\n            argmax_val = predictions_argmax[i]\n            decision = \"ДА\" if prob_vals[1] >= threshold else \"НЕТ\"\n            \n            print(f\"{i:<6} | {char:<6} | [{logit_vals[0]:>6.2f}, {logit_vals[1]:>6.2f}]      | [{prob_vals[0]:.2f}, {prob_vals[1]:.2f}]            | {argmax_val:<6} | {decision}\")\n\n            if decision == \"ДА\":\n                positions.append(word_idx + 1)\n                \n    final_positions = sorted(list(set(positions)))\n    print(f\"\\nИтоговый результат для порога {threshold}: {final_positions}\")\n    print(\"-\" * 80 + \"\\n\")\n    return final_positions\n    \nsome_test_sentences = [\n    \"приветмир\",\n    \"продамгараж\",\n    \"книгавхорошемсостоянии\",\n    \"надоподумать\"\n]\n\nfor sentence in some_test_sentences:\n    _ = debug_restore_positions(sentence, trainer.model, tokenizer, device, threshold=0.5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T22:25:41.438832Z","iopub.status.idle":"2025-09-25T22:25:41.439047Z","shell.execute_reply.started":"2025-09-25T22:25:41.438941Z","shell.execute_reply":"2025-09-25T22:25:41.438950Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm\nimport ast\nimport glob\nimport torch\nimport os\n\ndef restore_positions_char_level(text, model, tokenizer, device, max_len=128):\n    \"\"\"Предсказывает позиции пробелов для одной строки текста НА УРОВНЕ СИМВОЛОВ.\"\"\"\n    if not isinstance(text, str) or not text.strip():\n        return []\n\n    tokens = list(text)\n    inputs = tokenizer(\n        tokens,\n        is_split_into_words=True,\n        return_tensors=\"pt\",\n        max_length=max_len,\n        padding=\"max_length\",\n        truncation=True\n    )\n\n    word_ids_map = inputs.word_ids()\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n\n    model.eval()\n    with torch.no_grad():\n        logits = model(**inputs).logits\n\n    predictions = torch.argmax(logits, dim=2)[0].cpu().numpy()\n    \n    positions = []\n    for i, word_idx in enumerate(word_ids_map):\n        if word_idx is not None and predictions[i] >= 0.6:\n            positions.append(word_idx + 1)\n            \n    return sorted(list(set(positions)))\n\n\ndef calculate_f1(true_pos, pred_pos):\n    if isinstance(true_pos, str):\n        try: true_pos = ast.literal_eval(true_pos)\n        except: true_pos = []\n\n    true_set = set(true_pos)\n    pred_set = set(pred_pos)\n\n    if not pred_set and not true_set: return 1.0\n\n    tp = len(true_set.intersection(pred_set))\n    precision = tp / len(pred_set) if pred_set else 0.0\n    recall = tp / len(true_set) if true_set else 0.0\n\n    if precision + recall == 0: return 0.0\n    return 2 * (precision * recall) / (precision + recall)\n\nprint(\"--- ЭТАП 1: Оценка F1-меры на локальном тестовом наборе ---\")\nLOCAL_TEST_DIR = \"/kaggle/input/avito-task-segment/\"\nEVAL_SAMPLE_SIZE = 20000 \ntest_files = glob.glob(f\"{LOCAL_TEST_DIR}test_*.parquet\")\nif not test_files:\n    print(f\"Предупреждение: В папке '{LOCAL_TEST_DIR}' не найдено файлов test_*.parquet. Пропускаем оценку.\")\nelse:\n    try:\n        first_test_file = test_files[0]\n        print(f\"Для быстрой оценки будет использован один файл: {os.path.basename(first_test_file)}\")\n        local_test_df = pd.read_parquet(first_test_file)\n\n        sample_size = min(EVAL_SAMPLE_SIZE, len(local_test_df))\n        eval_sample_df = local_test_df.sample(n=sample_size, random_state=42)\n        print(f\"Используем сэмпл из {len(eval_sample_df)} строк для оценки F1.\")\n\n        texts_to_eval = eval_sample_df['sentence_without_spaces'].fillna('').tolist()\n\n        local_predicted_positions = [\n            restore_positions_char_level(text, trainer.model, tokenizer, device, max_len=MAX_LEN)\n            for text in tqdm(texts_to_eval, desc=\"Оценка F1 (инференс)\")\n        ]\n\n        f1_scores = [\n            calculate_f1(true_pos, pred_pos)\n            for true_pos, pred_pos in zip(eval_sample_df['true_positions'], local_predicted_positions)\n        ]\n\n        average_f1 = np.mean(f1_scores)\n        print(f\"\\n✅ Средняя F1-мера на СЭМПЛЕ локального теста: {average_f1:.4f}\")\n\n    except Exception as e:\n        print(f\"❌ ОШИБКА на ЭТАПЕ 1: Не удалось выполнить оценку. Детали: {e}\")\n\n\nprint(\"\\n--- ЭТАП 2: Генерация файла submission.csv для отправки ---\")\nSUBMISSION_SOURCE_PATH = \"/kaggle/input/avito-task-segment/submission.parquet\"\ntry:\n    submission_source_df = pd.read_parquet(SUBMISSION_SOURCE_PATH)\n    print(f\"Загружено {len(submission_source_df)} записей для генерации сабмита.\")\n\n    texts_to_submit = submission_source_df['text_no_spaces'].fillna('').tolist()\n\n    final_predicted_positions = [\n        restore_positions_char_level(text, trainer.model, tokenizer, device, max_len=MAX_LEN)\n        for text in tqdm(texts_to_submit, desc=\"Генерация сабмита\")\n    ]\n\n    final_submission_df = pd.DataFrame({\n        'id': submission_source_df['id'],\n        'predicted_positions': [str(p) for p in final_predicted_positions]\n    })\n    final_submission_df.to_csv('submission.csv', index=False)\n\n    print(\"\\n✅ Файл submission.csv успешно создан!\")\n    print(\"Пример содержимого:\")\n    print(final_submission_df.head())\n\nexcept FileNotFoundError:\n    print(f\"❌ ОШИБКА на ЭТАПЕ 2: Файл для сабмита не найден по пути '{SUBMISSION_SOURCE_PATH}'\")\nexcept Exception as e:\n    print(f\"❌ ОШИБКА на ЭТАПЕ 2: Не удалось обработать файл. Детали: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T22:25:41.440318Z","iopub.status.idle":"2025-09-25T22:25:41.440865Z","shell.execute_reply.started":"2025-09-25T22:25:41.440679Z","shell.execute_reply":"2025-09-25T22:25:41.440697Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(final_submission_df.head(40))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T22:25:41.442112Z","iopub.status.idle":"2025-09-25T22:25:41.442397Z","shell.execute_reply.started":"2025-09-25T22:25:41.442267Z","shell.execute_reply":"2025-09-25T22:25:41.442280Z"}},"outputs":[],"execution_count":null}]}